{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDANMF2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCunuT9JTwOg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6c62d7c7-fcd1-4d1a-b9e9-3b076b5372e4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install pyLDAvis\n",
        "!pip install --upgrade gensim\n",
        "%load_ext google.colab.data_table\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from collections import Counter\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "import spacy, gensim\n",
        "import nltk\n",
        "import re\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import spacy, gensim\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "nlp = spacy.load('en')\n",
        "import string\n",
        "from plotly.offline import plot\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from sklearn.decomposition.online_lda import LatentDirichletAllocation\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from gensim import corpora, models\n",
        "from collections import Counter\n",
        "import sklearn.feature_extraction.text as text\n",
        "from sklearn import decomposition\n",
        "from sklearn.decomposition import NMF\n",
        "import gensim\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from textblob import TextBlob\n",
        "from nltk.tag import pos_tag\n",
        "from sklearn.svm import SVC\n",
        "import heapq\n",
        "import warnings\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import re\n",
        "import numpy as np\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "import spacy\n",
        "# for plotting\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models\n",
        "import matplotlib.pyplot as pl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyLDAvis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/a5/15a0da6b0150b8b68610cc78af80364a80a9a4c8b6dd5ee549b8989d4b60/pyLDAvis-3.3.1.tar.gz (1.7MB)\n",
            "\r\u001b[K     |▏                               | 10kB 16.6MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 21.2MB/s eta 0:00:01\r\u001b[K     |▋                               | 30kB 24.7MB/s eta 0:00:01\r\u001b[K     |▉                               | 40kB 27.4MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 30.2MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61kB 32.6MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71kB 32.9MB/s eta 0:00:01\r\u001b[K     |█▋                              | 81kB 33.5MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92kB 29.4MB/s eta 0:00:01\r\u001b[K     |██                              | 102kB 28.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 112kB 28.7MB/s eta 0:00:01\r\u001b[K     |██▍                             | 122kB 28.7MB/s eta 0:00:01\r\u001b[K     |██▋                             | 133kB 28.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 143kB 28.7MB/s eta 0:00:01\r\u001b[K     |███                             | 153kB 28.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 163kB 28.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 174kB 28.7MB/s eta 0:00:01\r\u001b[K     |███▋                            | 184kB 28.7MB/s eta 0:00:01\r\u001b[K     |███▊                            | 194kB 28.7MB/s eta 0:00:01\r\u001b[K     |████                            | 204kB 28.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 215kB 28.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 225kB 28.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 235kB 28.7MB/s eta 0:00:01\r\u001b[K     |████▊                           | 245kB 28.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 256kB 28.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 266kB 28.7MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 276kB 28.7MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 286kB 28.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 296kB 28.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 307kB 28.7MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 317kB 28.7MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 327kB 28.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 337kB 28.7MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 348kB 28.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 358kB 28.7MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 368kB 28.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 378kB 28.7MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 389kB 28.7MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 399kB 28.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 409kB 28.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 419kB 28.7MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 430kB 28.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 440kB 28.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 450kB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 460kB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 471kB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 481kB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 491kB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 501kB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 512kB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 522kB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 532kB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 542kB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 552kB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 563kB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 573kB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 583kB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 593kB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 604kB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 614kB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 624kB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 634kB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 645kB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 655kB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 665kB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 675kB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 686kB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 696kB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 706kB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 716kB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 727kB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 737kB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 747kB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 757kB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 768kB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 778kB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 788kB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 798kB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 808kB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 819kB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 829kB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 839kB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 849kB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 860kB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 870kB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 880kB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 890kB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 901kB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 911kB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 921kB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 931kB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 942kB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 952kB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 962kB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 972kB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 983kB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 993kB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.0MB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.0MB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.0MB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.0MB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.0MB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1MB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.1MB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.1MB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1MB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.1MB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.1MB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.1MB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.1MB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1MB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1MB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2MB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.2MB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.2MB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2MB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.2MB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.2MB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.2MB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.2MB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2MB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.2MB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.3MB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.3MB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.3MB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3MB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.3MB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.3MB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.3MB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.3MB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3MB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.4MB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.4MB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.4MB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.4MB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.4MB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.4MB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.4MB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.4MB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.4MB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4MB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.5MB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.5MB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.5MB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.5MB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.5MB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.5MB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.5MB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.5MB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.5MB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5MB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.6MB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.6MB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.6MB 28.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.6MB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6MB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6MB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.6MB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.6MB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.6MB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6MB 28.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7MB 28.7MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.0.0)\n",
            "Collecting numpy>=1.20.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/42/560d269f604d3e186a57c21a363e77e199358d054884e61b73e405dd217c/numpy-1.20.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.3MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3MB 138kB/s \n",
            "\u001b[?25hCollecting pandas>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/51/48f3fc47c4e2144da2806dfb6629c4dd1fa3d5a143f9652b141e979a8ca9/pandas-1.2.4-cp37-cp37m-manylinux1_x86_64.whl (9.9MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9MB 32.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Collecting funcy\n",
            "  Downloading https://files.pythonhosted.org/packages/44/52/5cf7401456a461e4b481650dfb8279bc000f31a011d0918904f86e755947/funcy-1.16-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.7.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.22.2.post1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.1.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-cp37-none-any.whl size=136897 sha256=75b2bdb793de87101361076b16c8b213947904e6760e246ce95fba93a4d34ded\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/9c/fc/c6e00689d35c82cf96a8adc70edfe7ba7904374fdac3240ac2\n",
            "Successfully built pyLDAvis\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement numpy~=1.19.2, but you'll have numpy 1.20.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 1.2.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, pandas, funcy, pyLDAvis\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "Successfully installed funcy-1.16 numpy-1.20.3 pandas-1.2.4 pyLDAvis-3.3.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/52/f1417772965652d4ca6f901515debcd9d6c5430969e8c02ee7737e6de61c/gensim-4.0.1-cp37-cp37m-manylinux1_x86_64.whl (23.9MB)\n",
            "\u001b[K     |████████████████████████████████| 23.9MB 132kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.20.3)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.1.0)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning:\n",
            "\n",
            "The sklearn.decomposition.online_lda module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.decomposition. Anything that cannot be imported from sklearn.decomposition is now part of the private API.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning:\n",
            "\n",
            "Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/past/builtins/misc.py:4: DeprecationWarning:\n",
            "\n",
            "Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5_vhAGTT5xw"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrqGL8jeT8No"
      },
      "source": [
        "# with open('fradulent_emails.txt','r', encoding='latin1') as file:\n",
        "#   data=file.read()\n",
        "# print(data[:20000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j60RFIXvApW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44782dbb-4bc6-4b5e-d251-f9e341472f4b"
      },
      "source": [
        "df=pd.read_csv('drive/MyDrive/CDAC/email.csv')\n",
        "df1=pd.read_csv('drive/MyDrive/CDAC/email1.csv')\n",
        "df2=pd.read_csv('drive/MyDrive/CDAC/enron.csv')\n",
        "data=pd.concat([df,df1,df2],ignore_index=True)\n",
        "data=data['Body']\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18651,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3zAxYQGApJl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "outputId": "9570a263-02d5-461b-f35b-197f30897acd"
      },
      "source": [
        "data.dropna(inplace=True)\n",
        "data.columns=['body']\n",
        "data=pd.DataFrame(data)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0mmax_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.max_columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0mmax_colwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.max_colwidth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m         \u001b[0mshow_dimensions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.show_dimensions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.expand_frame_repr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_console_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_html\u001b[0;34m(self, buf, encoding, classes, notebook, border)\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0mnotebook\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0mWhether\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mHTML\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mIPython\u001b[0m \u001b[0mNotebook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m         \u001b[0mborder\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m             \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mborder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mborder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mattribute\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mincluded\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mopening\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefault\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mborder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NotebookFormatter' object has no attribute 'get_result'"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"\\nSave up to 70% on Life Insurance.\\nWhy Spend More Than You Have To?Life Quote Savings\\nEnsuring your \\n      family's financial security is very important. Life Quote Savings makes \\n      buying life insurance simple and affordable. We Provide FREE Access to The \\n      Very Best Companies and The Lowest Rates.Life Quote Savings is FAST, EASY and \\n            SAVES you money! Let us help you get started with the best values in \\n            the country on new coverage. You can SAVE hundreds or even thousands \\n            of dollars by requesting a FREE quote from Lifequote Savings. Our \\n            service will take you less than 5 minutes to complete. Shop and \\n            compare. SAVE up to 70% on all types of Life insurance! Click Here For Your \\n            Free Quote!Protecting your family is the best investment you'll ever \\n          make!\\nIf you are in receipt of this email \\n      in error and/or wish to be removed from our list, PLEASE CLICK HERE AND TYPE REMOVE. If you \\n      reside in any state which prohibits e-mail solicitations for insurance, \\n      please disregard this \\n      email.\\n\"],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n\"1) Fight The Risk of Cancer!\\nhttp://www.adclick.ws/p.cfm?o=315&s=pk0072) Slim Down - Guaranteed to lose 10-12 lbs in 30 days\\nhttp://www.adclick.ws/p.cfm?o=249&s=pk0073) Get the Child Support You Deserve - Free Legal Advice\\nhttp://www.adclick.ws/p.cfm?o=245&s=pk0024) Join the Web's Fastest Growing Singles Community\\nhttp://www.adclick.ws/p.cfm?o=259&s=pk0075) Start Your Private Photo Album Online!\\nhttp://www.adclick.ws/p.cfm?o=283&s=pk007Have a Wonderful Day,\\nOffer Manager\\nPrizeMamaIf you wish to leave this list please use the link below.\\nhttp://www.qves.com/trim/?ilug@linux.ie%7C17%7C114258\\n-- \\nIrish Linux Users' Group: ilug@linux.ie\\nhttp://www.linux.ie/mailman/listinfo/ilug for (un)subscription information.\\nList maintainer: listmaster@linux.ie\"],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n\"1) Fight The Risk of Cancer!\\nhttp://www.adclick.ws/p.cfm?o=315&s=pk0072) Slim Down - Guaranteed to lose 10-12 lbs in 30 days\\nhttp://www.adclick.ws/p.cfm?o=249&s=pk0073) Get the Child Support You Deserve - Free Legal Advice\\nhttp://www.adclick.ws/p.cfm?o=245&s=pk0024) Join the Web's Fastest Growing Singles Community\\nhttp://www.adclick.ws/p.cfm?o=259&s=pk0075) Start Your Private Photo Album Online!\\nhttp://www.adclick.ws/p.cfm?o=283&s=pk007Have a Wonderful Day,\\nOffer Manager\\nPrizeMamaIf you wish to leave this list please use the link below.\\nhttp://www.qves.com/trim/?zzzz@spamassassin.taint.org%7C17%7C308417\"],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n\"##################################################\\n#                                                #\\n#                 Adult Club                     #\\n#           Offers FREE Membership               #\\n#                                                #\\n##################################################>>>>>  INSTANT ACCESS TO ALL SITES NOW\\n>>>>>  Your User Name And Password is.\\n>>>>>  User Name: zzzz@spamassassin.taint.org\\n>>>>>  Password: 7603825 of the Best Adult Sites on the Internet for FREE!\\n---------------------------------------\\nNEWS 08/18/02\\nWith just over 2.9 Million Members that signed up for FREE, Last month there were 721,184 New\\nMembers. Are you one of them yet???\\n---------------------------------------\\nOur Membership FAQQ. Why are you offering free access to 5 adult membership sites for free?\\nA. I have advertisers that pay me for ad space so you don't have to pay for membership.Q. Is it true my membership is for life?\\nA. Absolutely you'll never have to pay a cent the advertisers do.Q. Can I give my account to my friends and family?\\nA. Yes, as long they are over the age of 18.Q. Do I have to sign up for all 5 membership sites?\\nA. No just one to get access to all of them.Q. How do I get started?\\nA. Click on one of the following links below to become a member.- These are multi million dollar operations with policies and rules.\\n- Fill in the required info and they won't charge you for the Free pass!\\n- If you don't believe us, just read their terms and conditions.---------------------------# 5. > Adults Farm\\nhttp://80.71.66.8/farm/?aid=760382\\nGirls and Animals Getting Freaky....FREE Lifetime Membership!!# 4. > Sexy Celebes\\nhttp://80.71.66.8/celebst/?aid=760382\\nThousands Of XXX Celebes doing it...FREE Lifetime Membership!!# 3. > Play House Porn\\nhttp://80.71.66.8/mega/?aid=760382\\nLive Feeds From 60 Sites And Web Cams...FREE Lifetime Membership!!# 2. > Asian Sex Fantasies\\nhttp://80.71.66.8/asian/?aid=760382\\nJapanese Schoolgirls, Live Sex Shows ...FREE Lifetime Membership!!# 1. > Lesbian Lace\\nhttp://80.71.66.8/lesbian/?aid=760382\\nGirls and Girls Getting Freaky! ...FREE Lifetime Membership!!--------------------------Jennifer Simpson, Miami, FL\\nYour FREE lifetime membership has entertained my boyffriend and I for\\nthe last two years!  Your Adult Sites are the best on the net!Joe Morgan Manhattan, NY\\nYour live sex shows and live sex cams are unbelievable. The best part\\nabout your porn sites, is that they're absolutely FREE!--------------------------Removal Instructions:You have received this advertisement because you have opted in to receive free adult internet\\noffers and specials through our affiliated websites. If you do not wish to receive further emails\\nor have received the email in error you may opt-out of our database here\\nhttp://80.71.66.8/optout/ . Please allow 24 hours for removal.vonolmosatkirekpups\"],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n\"I thought you might like these:\\n1) Slim Down - Guaranteed to lose 10-12 lbs in 30 days\\nhttp://www.freeyankee.com/cgi/fy2/to.cgi?l=822slim12) Fight The Risk of Cancer! \\nhttp://www.freeyankee.com/cgi/fy2/to.cgi?l=822nic1 3) Get the Child Support You Deserve - Free Legal Advice \\nhttp://www.freeyankee.com/cgi/fy2/to.cgi?l=822ppl1Offer Manager\\nDaily-Deals\\nIf you wish to leave this list please use the link below.\\nhttp://www.qves.com/trim/?social@linux.ie%7C29%7C134077\\n-- \\nIrish Linux Users' Group Social Events: social@linux.ie\\nhttp://www.linux.ie/mailman/listinfo/social for (un)subscription information.\\nList maintainer: listmaster@linux.ie\"]],\n        columns: [[\"number\", \"index\"], [\"string\", \"Body\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/plain": [
              "                                                Body\n",
              "0  \\nSave up to 70% on Life Insurance.\\nWhy Spend...\n",
              "1  1) Fight The Risk of Cancer!\\nhttp://www.adcli...\n",
              "2  1) Fight The Risk of Cancer!\\nhttp://www.adcli...\n",
              "3  ##############################################...\n",
              "4  I thought you might like these:\\n1) Slim Down ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u1aFZ9GBA_x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1015
        },
        "outputId": "daad5250-e26c-490b-dbdd-52ccdfff3135"
      },
      "source": [
        "data2=data\n",
        "body_length=[]\n",
        "for body in data2['Body']:\n",
        "  body_length.append(len(body))\n",
        "\n",
        "data2['body_length']=body_length\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0mmax_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.max_columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0mmax_colwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.max_colwidth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m         \u001b[0mshow_dimensions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.show_dimensions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.expand_frame_repr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_console_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_html\u001b[0;34m(self, buf, encoding, classes, notebook, border)\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0mnotebook\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0mWhether\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mHTML\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mIPython\u001b[0m \u001b[0mNotebook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m         \u001b[0mborder\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m             \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mborder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mborder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mattribute\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mincluded\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mopening\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefault\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mborder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NotebookFormatter' object has no attribute 'get_result'"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"\\nSave up to 70% on Life Insurance.\\nWhy Spend More Than You Have To?Life Quote Savings\\nEnsuring your \\n      family's financial security is very important. Life Quote Savings makes \\n      buying life insurance simple and affordable. We Provide FREE Access to The \\n      Very Best Companies and The Lowest Rates.Life Quote Savings is FAST, EASY and \\n            SAVES you money! Let us help you get started with the best values in \\n            the country on new coverage. You can SAVE hundreds or even thousands \\n            of dollars by requesting a FREE quote from Lifequote Savings. Our \\n            service will take you less than 5 minutes to complete. Shop and \\n            compare. SAVE up to 70% on all types of Life insurance! Click Here For Your \\n            Free Quote!Protecting your family is the best investment you'll ever \\n          make!\\nIf you are in receipt of this email \\n      in error and/or wish to be removed from our list, PLEASE CLICK HERE AND TYPE REMOVE. If you \\n      reside in any state which prohibits e-mail solicitations for insurance, \\n      please disregard this \\n      email.\\n\",\n{\n            'v': 1111,\n            'f': \"1111\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n\"1) Fight The Risk of Cancer!\\nhttp://www.adclick.ws/p.cfm?o=315&s=pk0072) Slim Down - Guaranteed to lose 10-12 lbs in 30 days\\nhttp://www.adclick.ws/p.cfm?o=249&s=pk0073) Get the Child Support You Deserve - Free Legal Advice\\nhttp://www.adclick.ws/p.cfm?o=245&s=pk0024) Join the Web's Fastest Growing Singles Community\\nhttp://www.adclick.ws/p.cfm?o=259&s=pk0075) Start Your Private Photo Album Online!\\nhttp://www.adclick.ws/p.cfm?o=283&s=pk007Have a Wonderful Day,\\nOffer Manager\\nPrizeMamaIf you wish to leave this list please use the link below.\\nhttp://www.qves.com/trim/?ilug@linux.ie%7C17%7C114258\\n-- \\nIrish Linux Users' Group: ilug@linux.ie\\nhttp://www.linux.ie/mailman/listinfo/ilug for (un)subscription information.\\nList maintainer: listmaster@linux.ie\",\n{\n            'v': 753,\n            'f': \"753\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n\"1) Fight The Risk of Cancer!\\nhttp://www.adclick.ws/p.cfm?o=315&s=pk0072) Slim Down - Guaranteed to lose 10-12 lbs in 30 days\\nhttp://www.adclick.ws/p.cfm?o=249&s=pk0073) Get the Child Support You Deserve - Free Legal Advice\\nhttp://www.adclick.ws/p.cfm?o=245&s=pk0024) Join the Web's Fastest Growing Singles Community\\nhttp://www.adclick.ws/p.cfm?o=259&s=pk0075) Start Your Private Photo Album Online!\\nhttp://www.adclick.ws/p.cfm?o=283&s=pk007Have a Wonderful Day,\\nOffer Manager\\nPrizeMamaIf you wish to leave this list please use the link below.\\nhttp://www.qves.com/trim/?zzzz@spamassassin.taint.org%7C17%7C308417\",\n{\n            'v': 610,\n            'f': \"610\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n\"##################################################\\n#                                                #\\n#                 Adult Club                     #\\n#           Offers FREE Membership               #\\n#                                                #\\n##################################################>>>>>  INSTANT ACCESS TO ALL SITES NOW\\n>>>>>  Your User Name And Password is.\\n>>>>>  User Name: zzzz@spamassassin.taint.org\\n>>>>>  Password: 7603825 of the Best Adult Sites on the Internet for FREE!\\n---------------------------------------\\nNEWS 08/18/02\\nWith just over 2.9 Million Members that signed up for FREE, Last month there were 721,184 New\\nMembers. Are you one of them yet???\\n---------------------------------------\\nOur Membership FAQQ. Why are you offering free access to 5 adult membership sites for free?\\nA. I have advertisers that pay me for ad space so you don't have to pay for membership.Q. Is it true my membership is for life?\\nA. Absolutely you'll never have to pay a cent the advertisers do.Q. Can I give my account to my friends and family?\\nA. Yes, as long they are over the age of 18.Q. Do I have to sign up for all 5 membership sites?\\nA. No just one to get access to all of them.Q. How do I get started?\\nA. Click on one of the following links below to become a member.- These are multi million dollar operations with policies and rules.\\n- Fill in the required info and they won't charge you for the Free pass!\\n- If you don't believe us, just read their terms and conditions.---------------------------# 5. > Adults Farm\\nhttp://80.71.66.8/farm/?aid=760382\\nGirls and Animals Getting Freaky....FREE Lifetime Membership!!# 4. > Sexy Celebes\\nhttp://80.71.66.8/celebst/?aid=760382\\nThousands Of XXX Celebes doing it...FREE Lifetime Membership!!# 3. > Play House Porn\\nhttp://80.71.66.8/mega/?aid=760382\\nLive Feeds From 60 Sites And Web Cams...FREE Lifetime Membership!!# 2. > Asian Sex Fantasies\\nhttp://80.71.66.8/asian/?aid=760382\\nJapanese Schoolgirls, Live Sex Shows ...FREE Lifetime Membership!!# 1. > Lesbian Lace\\nhttp://80.71.66.8/lesbian/?aid=760382\\nGirls and Girls Getting Freaky! ...FREE Lifetime Membership!!--------------------------Jennifer Simpson, Miami, FL\\nYour FREE lifetime membership has entertained my boyffriend and I for\\nthe last two years!  Your Adult Sites are the best on the net!Joe Morgan Manhattan, NY\\nYour live sex shows and live sex cams are unbelievable. The best part\\nabout your porn sites, is that they're absolutely FREE!--------------------------Removal Instructions:You have received this advertisement because you have opted in to receive free adult internet\\noffers and specials through our affiliated websites. If you do not wish to receive further emails\\nor have received the email in error you may opt-out of our database here\\nhttp://80.71.66.8/optout/ . Please allow 24 hours for removal.vonolmosatkirekpups\",\n{\n            'v': 2863,\n            'f': \"2863\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n\"I thought you might like these:\\n1) Slim Down - Guaranteed to lose 10-12 lbs in 30 days\\nhttp://www.freeyankee.com/cgi/fy2/to.cgi?l=822slim12) Fight The Risk of Cancer! \\nhttp://www.freeyankee.com/cgi/fy2/to.cgi?l=822nic1 3) Get the Child Support You Deserve - Free Legal Advice \\nhttp://www.freeyankee.com/cgi/fy2/to.cgi?l=822ppl1Offer Manager\\nDaily-Deals\\nIf you wish to leave this list please use the link below.\\nhttp://www.qves.com/trim/?social@linux.ie%7C29%7C134077\\n-- \\nIrish Linux Users' Group Social Events: social@linux.ie\\nhttp://www.linux.ie/mailman/listinfo/social for (un)subscription information.\\nList maintainer: listmaster@linux.ie\",\n{\n            'v': 641,\n            'f': \"641\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"string\", \"Body\"], [\"number\", \"body_length\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/plain": [
              "                                                Body  body_length\n",
              "0  \\nSave up to 70% on Life Insurance.\\nWhy Spend...         1111\n",
              "1  1) Fight The Risk of Cancer!\\nhttp://www.adcli...          753\n",
              "2  1) Fight The Risk of Cancer!\\nhttp://www.adcli...          610\n",
              "3  ##############################################...         2863\n",
              "4  I thought you might like these:\\n1) Slim Down ...          641"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guihPvY2UFPW"
      },
      "source": [
        "# import email\n",
        "\n",
        "# def extract_messages(df):\n",
        "#     messages = []\n",
        "#     for item in df[\"message\"]:\n",
        "#         # Return a message object structure from a string\n",
        "#         e = email.message_from_string(item)    \n",
        "#         # get message body  \n",
        "#         message_body = e.get_payload()\n",
        "#         messages.append(message_body)\n",
        "#     print(\"Successfully retrieved message body from e-mails!\")\n",
        "#     return messages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb6QGyXnUIga"
      },
      "source": [
        "# fraud_emails = data.split(\"From r\")\n",
        "# print(\"Successfully loaded {} spam emails!\".format(len(fraud_emails)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFpcrVwFUOup"
      },
      "source": [
        "# fraud_bodies = extract_messages(pd.DataFrame(fraud_emails,columns=[\"message\"],dtype=str))\n",
        "# x=np.arange(3978)\n",
        "# fraud_bodies_df = pd.DataFrame(fraud_bodies[1:])\n",
        "# fraud_bodies_df.columns=['Message']\n",
        "# pd.set_option('display.max_colwidth',300)\n",
        "# fraud_bodies_df.head() # you could do print(fraud_bodies_df.head()), but Jupyter displays this nicer for pandas DataFrames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olSvR3SPc8Xx"
      },
      "source": [
        "# fraud_ex=[]\n",
        "# for i in fraud_bodies_df['Message']:\n",
        "#   i=''.join(str(i))\n",
        "#   fraud_ex.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGLCS8JWdhIF"
      },
      "source": [
        "# fraud_df=pd.DataFrame(fraud_ex)\n",
        "# fraud_df.columns=['Message']\n",
        "# fraud_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgl220Zhe0vz"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "corpus=[]\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "for body in data2['Body']:\n",
        "  removed_newline_tab=re.sub('[\\n|\\t]',' ',body)\n",
        "  removed_sub=re.sub('Subject: ',' ',removed_newline_tab)\n",
        "  removed_spchar=re.sub('[^a-zA-z0-9]{3,}',' ',removed_sub)\n",
        "\n",
        "  lower_cased=removed_spchar.lower()\n",
        "  tokenized_email=lower_cased.split()\n",
        "\n",
        "  filtered_words=[word for word in tokenized_email if word not in stopwords.words('english')]\n",
        "  lemmatized_email=[lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "  body=' '.join(lemmatized_email)\n",
        "  corpus.append(body)\n",
        "  corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Uu4U0smr-V"
      },
      "source": [
        "# from gensim import corpora\n",
        "# corpus=corpora.MmCorpus('drive\\MyDrive\\CDAC\\corpus.mm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9hxNjdIXz6j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "1c87aa5f-ded1-41a8-e46a-79288f8fbb59"
      },
      "source": [
        "fraud_df=pd.DataFrame(corpus)\n",
        "# fraud_df.colunms=['Body']\n",
        "fraud_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0mmax_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.max_columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0mmax_colwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.max_colwidth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m         \u001b[0mshow_dimensions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.show_dimensions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.expand_frame_repr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_console_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_html\u001b[0;34m(self, buf, encoding, classes, notebook, border)\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0mnotebook\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0mWhether\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mHTML\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mIPython\u001b[0m \u001b[0mNotebook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m         \u001b[0mborder\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m             \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mborder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mborder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mattribute\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mincluded\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mopening\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefault\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mborder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NotebookFormatter' object has no attribute 'get_result'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN17hZOIameq"
      },
      "source": [
        "def clean_text(text):\n",
        "  text=text.lower()\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'\\[.*?\\]', '', text)\n",
        "  text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
        "  text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "  return text\n",
        "\n",
        "fraud_df=pd.DataFrame(fraud_df.body.apply(lambda x:clean_text(x)))\n",
        "fraud_df.head()\n",
        "fraud_df.to_csv('file.csv',index=False)\n",
        "# fraud_clean=pd.read_csv('file.csv')\n",
        "# fraud_clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqT5Sm0PzLSf"
      },
      "source": [
        "fraud_ex1=[]\n",
        "for i in fraud_df['Message']:\n",
        "  i=''.join(str(i))\n",
        "  fraud_ex1.append(i)\n",
        "  # print(type(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D91qVQafzUDf"
      },
      "source": [
        "fraud_df1=pd.DataFrame(fraud_ex1)\n",
        "fraud_df1.columns=['Message']\n",
        "fraud_df1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygh7xcU9e1st"
      },
      "source": [
        "def lemmatizer(text):\n",
        "  sent=[]\n",
        "  doc=nlp(text)\n",
        "  for word in doc:\n",
        "    sent.append(word.lemma_)\n",
        "  return ' '.join(sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hc_Jhx60-iF"
      },
      "source": [
        "lemmatized_corpus=[]\n",
        "for i in range(len(fraud_df1)):\n",
        "  text_lemma=lemmatizer(fraud_df1['Message'][i])\n",
        "  lemmatized_corpus.append(text_lemma)\n",
        "\n",
        "lemmatized_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnaQIEOc5zmW"
      },
      "source": [
        "lemmatized_corpus=pd.DataFrame(lemmatized_corpus)\n",
        "lemmatized_corpus.columns=['Message']\n",
        "lemmatized_corpus.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrBkUJuh99OZ"
      },
      "source": [
        "def clean_it(text):\n",
        "  text=re.sub(r'-PRON-','',text)\n",
        "  text=re.sub(r'\\n','',text)\n",
        "  return text\n",
        "lemmatized_corpus=pd.DataFrame(lemmatized_corpus.Message.apply(lambda x:clean_it(x)))\n",
        "lemmatized_corpus['Message'][2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJo73HTg5IxV"
      },
      "source": [
        "# lemmatized_corpus=pd.DataFrame(lemmatized_corpus.Message.apply(lambda x:clean_text(x)))\n",
        "# lemmatized_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYjKrEDyvSe4"
      },
      "source": [
        "stop_words=stopwords.words('english')\n",
        "stop_words.extend(['from','account','bank','transfer','money'])\n",
        "\n",
        "vectorizer=CountVectorizer(analyzer='word',\n",
        "                           min_df=3,\n",
        "                           stop_words='english',\n",
        "                           lowercase=True,\n",
        "                           token_pattern='[a-zA-Z0-9]{3,}',\n",
        "                           max_features=1000,\n",
        "                           )\n",
        "\n",
        "data_vectorized=vectorizer.fit_transform(lemmatized_corpus['Message'])\n",
        "lda_model = LatentDirichletAllocation(n_components=7, # Number of topics\n",
        "                                      learning_method='online',\n",
        "                                      random_state=0,       \n",
        "                                      n_jobs = -1  # Use all available CPUs\n",
        "                                     )\n",
        "lda_output = lda_model.fit_transform(data_vectorized)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etrdaLXkYDrc"
      },
      "source": [
        "lda_model1=gensim.models.LdaMulticore(corpus=corpus,id2word=id2word,num_topics=7,random_state=100,chunksize=100,passes=10,per_word_topics=True)\n",
        "lda_model1.print_topics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCWPw4iYCor1"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeLgPnt-9x04"
      },
      "source": [
        "# import gensim\n",
        "# from gensim.utils import simple_preprocess\n",
        "# copy1=lemmatized_corpus\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "data = lemmatized_corpus.Message.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1][0][:30])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVKU5GgJ-__e"
      },
      "source": [
        "corpus = [id2word.doc2bow(text) for text in data_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPfxGs5bbGgn"
      },
      "source": [
        "# from gensim.corpora import corpora\n",
        "\n",
        "id2word=corpora.Dictionary(data_words)\n",
        "\n",
        "coherence_model_lda=CoherenceModel(model=lda_model,texts=data_words,dictionary=id2word,coherence='c_v')\n",
        "coherence_lda=coherence_model_lda.get_coherence()\n",
        "\n",
        "print('\\nCoherence Score: ',coherence_lda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCHAY6VAKG4U"
      },
      "source": [
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "    \"\"\"\n",
        "    Compute c_v coherence for various number of topics\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    dictionary : Gensim dictionary\n",
        "    corpus : Gensim corpus\n",
        "    texts : List of input texts\n",
        "    limit : Max num of topics\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    model_list : List of LDA topic models\n",
        "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFerX_H3MbPa"
      },
      "source": [
        "# from gensim.models.ldamodel import Ldamodel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbHKKdLDKYUP"
      },
      "source": [
        "dictionary=Dictionary(data_words)\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=data_words, start=2, limit=40, step=6)\n",
        "# Show graph\n",
        "import matplotlib.pyplot as plt\n",
        "limit=40; start=2; step=6;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyRrhtKvAnHa"
      },
      "source": [
        "# supporting function\n",
        "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
        "    \n",
        "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                           id2word=dictionary,\n",
        "                                           num_topics=k, \n",
        "                                           random_state=100,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha=a,\n",
        "                                           eta=b)\n",
        "    \n",
        "    coherence_model_lda = CoherenceModel(model=lda_model_copy1, texts=data_words, dictionary=id2word, coherence='c_v')\n",
        "    \n",
        "    return coherence_model_lda.get_coherence()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTuBdGa0BTHu"
      },
      "source": [
        "import numpy as np\n",
        "import tqdm\n",
        "grid = {}\n",
        "grid['Validation_Set'] = {}\n",
        "# Topics range\n",
        "min_topics = 2\n",
        "max_topics = 11\n",
        "step_size = 1\n",
        "topics_range = range(min_topics, max_topics, step_size)\n",
        "# Alpha parameter\n",
        "alpha = list(np.arange(0.01, 1, 0.3))\n",
        "alpha.append('symmetric')\n",
        "alpha.append('asymmetric')\n",
        "# Beta parameter\n",
        "beta = list(np.arange(0.01, 1, 0.3))\n",
        "beta.append('symmetric')\n",
        "# Validation sets\n",
        "num_of_docs = len(corpus)\n",
        "corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n",
        "               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n",
        "               gensim.utils.ClippedCorpus(corpus, num_of_docs*0.75), \n",
        "               corpus]\n",
        "corpus_title = ['75% Corpus', '100% Corpus']\n",
        "model_results = {'Validation_Set': [],\n",
        "                 'Topics': [],\n",
        "                 'Alpha': [],\n",
        "                 'Beta': [],\n",
        "                 'Coherence': []\n",
        "                }\n",
        "# Can take a long time to run\n",
        "if 1 == 1:\n",
        "    pbar = tqdm.tqdm(total=540)\n",
        "    \n",
        "    # iterate through validation corpuses\n",
        "    for i in range(len(corpus_sets)):\n",
        "        # iterate through number of topics\n",
        "        for k in topics_range:\n",
        "            # iterate through alpha values\n",
        "            for a in alpha:\n",
        "                # iterare through beta values\n",
        "                for b in beta:\n",
        "                    # get the coherence score for the given parameters\n",
        "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n",
        "                                                  k=k, a=a, b=b)\n",
        "                    # Save the model results\n",
        "                    model_results['Validation_Set'].append(corpus_title[i])\n",
        "                    model_results['Topics'].append(k)\n",
        "                    model_results['Alpha'].append(a)\n",
        "                    model_results['Beta'].append(b)\n",
        "                    model_results['Coherence'].append(cv)\n",
        "                    \n",
        "                    pbar.update(1)\n",
        "    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\n",
        "    pbar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBb_wWHzCvBd"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZkZZWPhA5E8"
      },
      "source": [
        "\n",
        "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n",
        "    keywords = np.array(vectorizer.get_feature_names())\n",
        "    topic_keywords = []\n",
        "    for topic_weights in lda_model.components_:\n",
        "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
        "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
        "    return topic_keywords\n",
        "\n",
        "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20)        \n",
        "\n",
        "# Topic - Keywords Dataframe\n",
        "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
        "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
        "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
        "df_topic_keywords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frKMkrpKAB1M"
      },
      "source": [
        "#Define labels for topics discovered "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFf7S6VyBQw_"
      },
      "source": [
        "#Getting  Tf-Idf vectorizer for our NMF models \n",
        "\n",
        "n_features = 2000\n",
        "n_components = 7\n",
        "n_top_words = 20\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
        "                                   max_features=n_features,\n",
        "                                   stop_words='english')\n",
        "tfidf = tfidf_vectorizer.fit_transform(lemmatized_corpus['Message'])\n",
        "# nmf = NMF(n_components=n_components, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0szUILAqB9mW"
      },
      "source": [
        "from sklearn.decomposition import NMF\n",
        "nmf = NMF(n_components=n_components, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZcjSwbSCOwJ"
      },
      "source": [
        "def show_topics(vectorizer=tfidf_vectorizer, nmf_model=nmf, n_words=20):\n",
        "    keywords = np.array(vectorizer.get_feature_names())\n",
        "    topic_keywords = []\n",
        "    for topic_weights in nmf_model.components_:\n",
        "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
        "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
        "    return topic_keywords\n",
        "\n",
        "topic_keywords = show_topics(vectorizer=tfidf_vectorizer, nmf_model=nmf, n_words=20)  \n",
        "# Topic - Keywords Dataframe\n",
        "df_topic_keywords1 = pd.DataFrame(topic_keywords)\n",
        "df_topic_keywords1.columns = ['Word '+str(i) for i in range(df_topic_keywords1.shape[1])]\n",
        "df_topic_keywords1.index = ['Topic '+str(i) for i in range(df_topic_keywords1.shape[0])]\n",
        "df_topic_keywords1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3iDJAkNWYCa"
      },
      "source": [
        "pyLDAvis.enable_notebook(sort=True)\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIMf0HZAPHBu"
      },
      "source": [
        "nmf_output=nmf.transform(tfidf)\n",
        "nmf_output[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL8hCUtc2B84"
      },
      "source": [
        "#Label NMF topics "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17m6WQvzJ3ZH"
      },
      "source": [
        "v=lda_output\n",
        "v=v*100\n",
        "len(v)\n",
        "v[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8-tz34aOe9S"
      },
      "source": [
        "from tqdm import tqdm\n",
        "s=[]\n",
        "for i in tqdm(range(len(nmf_output))):\n",
        "  s1=nmf_output[i]/sum(nmf_output[i])\n",
        "  s.append(s1)\n",
        "\n",
        "nmf_output=np.array(s)\n",
        "d=nmf_output\n",
        "d=d*100\n",
        "len(d)\n",
        "d[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaCePnBoRvj9"
      },
      "source": [
        "def normalize(df):\n",
        "  result=df.copy()\n",
        "  for feature_name in df.columns:\n",
        "    max_value=df[feature_name].max()\n",
        "    min_value=df[feature_name].min()\n",
        "\n",
        "    if max_value - min_value !=0:\n",
        "      result[feature_name]=(df[feature_name]-min_value) / (max_value - min_value) \n",
        "    else:\n",
        "      result[feature_name]=0\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_P9q4nyTrGI"
      },
      "source": [
        "LDA_df=pd.DataFrame(v,columns=df_topic_keywords.T.columns)\n",
        "NMF_df=pd.DataFrame(d,columns=df_topic_keywords1.T.columns)\n",
        "\n",
        "LDA_normalized=normalize(LDA_df)\n",
        "NMF_normalized=normalize(NMF_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN-M1XMbVrQP"
      },
      "source": [
        "LDANMF=pd.concat([NMF_normalized,LDA_normalized],axis=1)\n",
        "LDANMF.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avko-KMmWfJT"
      },
      "source": [
        "def compute_confidence_score(similarityList):\n",
        "  similarityscores=set(similarityList)\n",
        "  highest=max(similarityscores)\n",
        "\n",
        "  similarityscores.remove(highest)\n",
        "\n",
        "  if(len(similarityscores)==0):\n",
        "    return 0\n",
        "  \n",
        "  second_highest=max(similarityscores)\n",
        "  return ((highest - second_highest)/(highest))*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueUvLh-eY4fd"
      },
      "source": [
        "dominant_topic=np.argmax(LDANMF.values,axis=1)\n",
        "\n",
        "LDANMF['confidence']=LDANMF.apply(lambda row:compute_confidence_score(row),axis=1)\n",
        "LDANMF['dominant_topic']=dominant_topic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hC1DTFhaLeQ"
      },
      "source": [
        "final=pd.concat([lemmatized_corpus,LDANMF[['dominant_topic','confidence']]],axis=1)\n",
        "final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6EZSnvOay4g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}